---
title: 'ST 541 Final Project: Analysis of Diabetes Risk Factors'
author: "Benjamin A. Ajibade and Jacob E. Waggoner"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE)

library("PerformanceAnalytics")
library(corrplot)
library(ggplot2)
theme_update(plot.title = element_text(hjust = 0.5))

df <- read.csv("diabetes.csv")
```

# Introduction

  Diabetes is a very prevelent disease that affects the human body's core functions of getting energy from food. In the United States, more than 37 million adults have diabetes, and it is the seventh-leading cause of death. Concerningly, 1 of 5 adults with diabetes are not aware they have the disease. A lack of knowledge and action for diabetes patients leads to an increased liklihood of extreme complications such as hear disease, kidney disease, vision loss, and death.  
  As a result of the severity of this disease, it is important that individuals are screened and informed of their risk level. This is important because diabetes can be present in a person without any obvious symptoms for a long period of time, and early detection allows for better outcomes. Understanding of the risk levels for those without diabetes is also important, because preventative measures can be taken before the disease is contracted.  
  The objective of this study is to determine which clinical measurements and records make individuals more or less susceptible to diabetes. Our key research question is as follows:

> How can clinical datapoints be used to determine an individual's risk of having diabetes?

  The methodology we used to accomplish this goal was based around performing a logistic regression to attain a ratio, between 0 and 1, that a person would be diagnosed with diabetes (outcome 1). Before constructing and assessing this model, we conducted data visualizations and statistical tests that would help us understand the data and what outcomes we should expect from our model. The data used to perform this analysis and modeling comes from the United States' National Institute of Diabetes and Digestive and Kidney Diseases and was downloaded from Kaggle.


# Data Exploration

## Variables

  The dataset contains 8 numeric predictors and one binary outcome. This aligns with the requrients for multiple logistic regression. Descriptions of each variable are:
  
* **Pregnancies**: Number of times pregnant
* **Glucose**: Plasma glucose concentration from an oral glucose tolerance test
* **Blood-Pressure**: Diastolic blood pressure (mm Hg)
* **Skin-Thickness**: Triceps skin fold thickness (mm)
* **Insulin**: 2-Hour serum insulin level (mu U/ml)
* **BMI**: Body mass index (weight in kg/ (height in m) ^2)
* **DiabetesPedigreeFunction**: Likelihood of diabetes based on family history
* **Age**: Age (in years) of the individual in observation
* **Outcome**: The target variable, a binary variable of 0 or 1, in which 1 is interpreted as "tested positive for diabetes"

  The first step in our data exploration was to assess the data quality and structure. We viewed he first few rows of the dataset to visually assess the data. We can see that out of our eight numeric variables, only two, BMI and the pedigree function, are continuous, and the rest are discrete.

```{r}
head(df)
```


  We also generated summary statistics of the data, as shown below. In addition to explaining the distributions and central tendancies of each variable, the output also demonstrated that the dataset contained no null values.

```{r}
summary(df)
```

## Variable Distributions

  Our next step in data exploration was to assess the distributions of all our variables. We generated the following histograms for this assessment.

```{r}
vars <- colnames(df)

par(mfrow=c(3,3))
for (i in 1:9) {
  hist(df[,i], xlab = vars[i], main = paste("Histogram of",vars[i]))
}
```

  From these visualizations, we determined that Pregnancies, SkinThickness, Insulin, DiabetesPedigree, and Age were right-skewed and that Glucose, Blood Pressure, and BMI were approximately normal. We observed an irregularity inthe glucose distribution in which there were a few values at 0, far from the normal range of that distribution. It would seem counterintuitive that a human could survive with such a low glucose level, and these may be an indicator of a missing measurement. We also observed in the "Pregnancies" histogram that the mode and a large portion of the observations had zero pregnancies. This suggests that the sample, of which we do not have extensive information about, does include both men and women.

## Interactions with Target

  Following our assessment of the variables within the entire sample, we then visualized the variables by target class to see which variables differed greatly in those who weren't diagnosed with diabetes, and those who were. This gave us an initial idea of which variables would provide value as a predictor, based on which have less overlap between the central areas of the boxplot between the two outcomes.

```{r, out.width = "3in"}
par(mar=c(10,10,2,2))
for (i in 1:8) {
  print(ggplot(data=df,aes_string(y=vars[i]))+
          geom_boxplot()+
          facet_wrap(~Outcome)+
          ggtitle(paste("Boxplot of",vars[i], "by Outcome"))
        )
}
```

# Statistical Tests

## Differences Between Outcome Groups

  Building upon our previous exploratory analysis, we wanted to assess which predictors had statistically significant differences in the means between the two outcome classes. This is an objective approach to glean similar information from the class boxplots in the previous section and will give us another indication of which predictors will be useful. To test this, we utilized Welch Two-Sample T-Tests for each predictor. The results are shown below.
  
```{r}
for (i in 1:8) {
  print(t.test(formula(paste("df$",vars[i],"~","df$Outcome",sep=""))))
}
```

  Six of the eight variables resulted in p-values below the .05 alpha level in our T-tests. This indicated that the null hypothesis was rejected, and there is a statistically significant difference between the means for a positive diabetes result (1) and a negative diabetes result (0). For Blood Pressure and Skin Thickness, the p-value was higher than .05 and the null hypothesis was not rejected, indicating that there is no difference between the means for each outcome group. As a result, these two predictors will probably be less valuable for predicting the diabetes outcome than the six with a statistically significant difference in means.

## Correlation Between Predictors

  We were also interested in whether any of the predictors had cross-correlations that may be of note when building our model. To do this, we calculated correlation values between all predictors. We also plotted these to provide a visual representation. We can see that most of our predictors have a low or non-existant correlation coeffecient; however, the correlation between BMI and Pregnancies stands out as it is the highest at .62.
  
```{r}
#remove outcome
df_corr<- df[,1:8]

library(ggcorrplot)
c2 <- cor_pmat(df_corr)
ggcorrplot(c2, type = 'upper', title='Correlation of Predictors', lab=TRUE)
```

# Statistical Modeling

After concluding our exploratory analysis, we moved on to developing a statistical model to explain how each clinical predictor related to the diabetes outcome, and to calculate risk scores for our sample.

## Model Selection

### Full Model

We started by plugging in all predictors into the model
```{r}
m1 <- glm(Outcome ~. ,family = binomial(), data=df)
summary(m1)
```
 Despite being significant the DiabetesPedigreeFunction has a high standard error. A quick look at the variance inflation factor for the predictor shows that there is no redundant predictor.
```{r}
library(car)
vif(m1)

```

## Backward Selection, BIC Criterion

Backward elimination starts with all potential predictor variables in the regression model.This amounts to deleting the predictor with the largest p -value each time. BIC has a strict penalty and we expect a reduced number of predictors with good significance at the end of the process. Final model: Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction

```{r}

n <- length(df$Outcome)
backBIC <- step(m1, direction="backward", data=df, k=log(n))
backBIC$coefficients
```
### Stepwise Selection, BIC Criterion
  
  Using BIC criteria Stepwise selects 4 variables: Pregnancies, Glucose, BMI and DiabetesPedigreeFunction. Procedure starts with no potential predictor variables in the regression equation. Then, it adds the predictor with the smallest p-value. Next, it adds second predictor with smallest p-value while checking if we can drop any previously added variable. This process is continued until adding an additional predictor does not yield P-value below requirement.
Final model: Outcome ~ Pregnancies + Glucose + BMI + DiabetesPedigreeFunction

```{r}
stepwiseBIC <- step(m1, direction="both", data=df, k=log(n))
stepwiseBIC$coefficients
```
 The Backward selection BIC and Hybrid BIC arrives at the same predictors.
 

## What is the final model of your recommendation?
  Backward selection with BIC selected 4 variables  and BIC selected 4 variables.Any of the two BIC is preferred because both models have similarly adjusted R-squared and anyone with less variables is preferred.

## Reduced model based on the most significant predictors
```{r}

m2 <- glm(Outcome~Pregnancies+Glucose + BloodPressure+BMI+DiabetesPedigreeFunction,family = binomial(), data=df)
summary(m2)
```
```{r}
#difference between the two models
anova(m2,m1,test="Chisq")
```
 All of the regression coefficients in the  model are now highly significant at the 5% level. The coefficients of the predictors Pregnancies, Glucose, BMI and DiabetesPedigreeFunction are positive implying that (all other things equal) higher Pregnancies, Glucose, BMI and DiabetesPedigreeFunction increases the chance of being diabetes patient as one would expect. However the coefficient of BloodPressure is negative.

## Model Adequacy
Marginal model plots for model

```{r warning=FALSE}
library(alr4)
mmps(m2,layout=c(2,3))
```
```{r}
## goodness-of-fit
library(ResourceSelection)
hoslem.test(df$Outcome, fitted(m2), g=10)
```


## Model Accuracy

### ROC
```{r}
# prediction 
pred <- predict(m2, type="response")
head(cbind(df$Outcome, pred))

# classification table / confusion matrix
table(round(pred))
sum(pred>=0.5)

# table
class.outcome <- data.frame(response = df$Outcome, predicted = round(pred,0))
xtabs<-xtabs(~ predicted + response, data = class.outcome)
xtabs
```
  Out of 768 observations, We see that the model predicted 555 zeros and 213 ones. Out of the 213 actual diabetes patient, The model correctly predicted 154(True positive 72.3) and misclassified 59(False Positive 27.7). Also for the other part, the model correctly predicted 441 patient without diabetes(True Negative 79.5) and wrongly predicted 114 (False Negative 20.5)

```{r}
# ROC curve/ AUC
par(mfrow=c(1,1))
library(pROC)
outcome.roc <- roc(df$Outcome ~ pred, plot = TRUE, print.auc = TRUE)
```


# Conclusion

In summary, we see that higher number of Pregnancies, Glucoselevel, BMI and DiabetesPedigreeFunction increases the chance of being diabetes greatly (with diabetePedegree being the most significant factor), while a low low blood pressure increases an individual's chances of being diagnosed with diabetes. This information would be valuable to researchers and medical practitioners looking for early warning signs of diabetes. Individuals who are aware that they have these high-risk traits could take more preventative measures and be screened for diabetes more often. The model as a whole could be used in the future to develop diabetes risk scores for medical patients. If an individual sees their risk score is getting closer to 1, or rising in that direction, they should be taking more preventative measures. Some limitations of this model for application in a clinical setting is that individuals may not have all the measurements that are necessary. Some of the predictors, such as glucose and insulin, are not measured frequently in doctors' visits unless the patient's risk of certain diseases is high. In conclusion, at AUC 83.7, the model is a relatively accurate to predict whether an individual has diabetes and can be used to improve medical outcomes.

# References

Data: https://www.kaggle.com/datasets/mathchi/diabetes-data-set

Diabetes Information: https://www.cdc.gov/diabetes/basics/diabetes.html

# Appendix


```{r, include = TRUE, results=FALSE}
print('Hello World')
  
      ```